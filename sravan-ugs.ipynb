{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "\n# Glue Studio Notebook\nYou are now running a **Glue Studio** notebook; before you can start using your notebook you *must* start an interactive session.\n\n## Available Magics\n|          Magic              |   Type       |                                                                        Description                                                                        |\n|-----------------------------|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n| %%configure                 |  Dictionary  |  A json-formatted dictionary consisting of all configuration parameters for a session. Each parameter can be specified here or through individual magics. |\n| %profile                    |  String      |  Specify a profile in your aws configuration to use as the credentials provider.                                                                          |\n| %iam_role                   |  String      |  Specify an IAM role to execute your session with.                                                                                                        |\n| %region                     |  String      |  Specify the AWS region in which to initialize a session.                                                                                                 |\n| %session_id                 |  String      |  Returns the session ID for the running session.                                                                                                          |\n| %connections                |  List        |  Specify a comma separated list of connections to use in the session.                                                                                     |\n| %additional_python_modules  |  List        |  Comma separated list of pip packages, s3 paths or private pip arguments.                                                                                 |\n| %extra_py_files             |  List        |  Comma separated list of additional Python files from S3.                                                                                                 |\n| %extra_jars                 |  List        |  Comma separated list of additional Jars to include in the cluster.                                                                                       |\n| %number_of_workers          |  Integer     |  The number of workers of a defined worker_type that are allocated when a job runs. worker_type must be set too.                                          |\n| %glue_version               |  String      |  The version of Glue to be used by this session. Currently, the only valid options are 2.0 and 3.0 (eg: %glue_version 2.0).                               |\n| %security_config            |  String      |  Define a security configuration to be used with this session.                                                                                            |\n| %sql                        |  String      |  Run SQL code. All lines after the initial %%sql magic will be passed as part of the SQL code.                                                            |\n| %streaming                  |  String      |  Changes the session type to Glue Streaming.                                                                                                              |\n| %etl                        |  String      |  Changes the session type to Glue ETL.                                                                                                                    |\n| %status                     |              |  Returns the status of the current Glue session including its duration, configuration and executing user / role.                                          |\n| %stop_session               |              |  Stops the current session.                                                                                                                               |\n| %list_sessions              |              |  Lists all currently running sessions by name and ID.                                                                                                     |\n| %worker_type                |  String      |  Standard, G.1X, *or* G.2X. number_of_workers must be set too. Default is G.1X.                                                                           |\n| %spark_conf                 |  String      |  Specify custom spark configurations for your session. E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer.                      |",
			"metadata": {
				"editable": false,
				"deletable": false,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 47,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "data=spark.read.csv(\"s3://sravan-capstone/ugs /usgs_main.csv\",header=True,inferSchema=True)\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 48,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "data.describe()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 49,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[summary: string, latitude: string, longitude: string, depth: string, mag: string, magType: string, nst: string, gap: string, dmin: string, rms: string, net: string, id: string, place: string, type: string, horizontalError: string, depthError: string, magError: string, magNst: string, status: string, locationSource: string, magSource: string]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "data.groupby(\"type\").count().show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 50,
			"outputs": [
				{
					"name": "stdout",
					"text": "+------------------+-----+\n|              type|count|\n+------------------+-----+\n|         explosion|  376|\n|         ice quake|   11|\n|      quarry blast|  665|\n|       other event|    5|\n|        earthquake|74752|\n|chemical explosion|    1|\n+------------------+-----+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "data.printSchema()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 51,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- time: timestamp (nullable = true)\n |-- latitude: double (nullable = true)\n |-- longitude: double (nullable = true)\n |-- depth: double (nullable = true)\n |-- mag: double (nullable = true)\n |-- magType: string (nullable = true)\n |-- nst: double (nullable = true)\n |-- gap: double (nullable = true)\n |-- dmin: double (nullable = true)\n |-- rms: double (nullable = true)\n |-- net: string (nullable = true)\n |-- id: string (nullable = true)\n |-- updated: timestamp (nullable = true)\n |-- place: string (nullable = true)\n |-- type: string (nullable = true)\n |-- horizontalError: double (nullable = true)\n |-- depthError: double (nullable = true)\n |-- magError: double (nullable = true)\n |-- magNst: double (nullable = true)\n |-- status: string (nullable = true)\n |-- locationSource: string (nullable = true)\n |-- magSource: string (nullable = true)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "data.show(5,False)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 53,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------------------+----------+------------+-----+----+-------+----+-----+--------+----+---+------------+-----------------------+----------------------------------+----------+---------------+----------+--------+------+---------+--------------+---------+\n|time                   |latitude  |longitude   |depth|mag |magType|nst |gap  |dmin    |rms |net|id          |updated                |place                             |type      |horizontalError|depthError|magError|magNst|status   |locationSource|magSource|\n+-----------------------+----------+------------+-----+----+-------+----+-----+--------+----+---+------------+-----------------------+----------------------------------+----------+---------------+----------+--------+------+---------+--------------+---------+\n|2022-03-04 21:28:02.44 |38.7596664|-122.7196655|1.61 |1.24|md     |14.0|115.0|0.004494|0.04|nc |nc73701241  |2022-03-04 21:29:36.906|3km SW of Anderson Springs, CA    |earthquake|0.3            |0.36      |0.1     |5.0   |automatic|nc            |nc       |\n|2022-03-04 21:27:28.19 |38.8338318|-122.8154984|1.82 |1.13|md     |22.0|66.0 |0.01632 |0.02|nc |nc73701236  |2022-03-04 21:29:02.128|8km NW of The Geysers, CA         |earthquake|0.19           |0.53      |0.14    |4.0   |automatic|nc            |nc       |\n|2022-03-04 21:25:05.13 |35.5966682|-120.2713318|11.57|2.31|md     |5.0 |178.0|0.159   |0.01|nc |nc73701231  |2022-03-04 21:26:56.893|11km SE of Shandon, CA            |earthquake|3.14           |3.51      |0.76    |3.0   |automatic|nc            |nc       |\n|2022-03-04 21:20:43.59 |35.9291667|-117.6608333|3.25 |0.88|ml     |9.0 |73.0 |0.02053 |0.13|ci |ci40199696  |2022-03-04 21:24:10.449|22km E of Little Lake, CA         |earthquake|0.33           |0.74      |0.055   |10.0  |automatic|ci            |ci       |\n|2022-03-04 21:19:08.215|62.3602   |-149.6345   |9.8  |1.4 |ml     |null|null |null    |0.52|ak |ak0222wjh21x|2022-03-04 21:22:15.794|24 km NNE of Susitna North, Alaska|earthquake|null           |0.5       |null    |null  |automatic|ak            |ak       |\n+-----------------------+----------+------------+-----+----+-------+----+-----+--------+----+---+------------+-----------------------+----------------------------------+----------+---------------+----------+--------+------+---------+--------------+---------+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "data=data.na.drop()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 54,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "data=data.drop(\"place\",\"time\",\"magSource\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 55,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "data=data.withColumnRenamed(\"updated\",\"time\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 56,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "data=data.withColumnRenamed(\"locationSource\",\"source\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 57,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "data.show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 58,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------------+-------------------+-----+----+-------+----+-----+--------+----+---+----------+--------------------+------------+---------------+----------+-----------------+------+---------+------+\n|         latitude|          longitude|depth| mag|magType| nst|  gap|    dmin| rms|net|        id|                time|        type|horizontalError|depthError|         magError|magNst|   status|source|\n+-----------------+-------------------+-----+----+-------+----+-----+--------+----+---+----------+--------------------+------------+---------------+----------+-----------------+------+---------+------+\n|       38.7596664|       -122.7196655| 1.61|1.24|     md|14.0|115.0|0.004494|0.04| nc|nc73701241|2022-03-04 21:29:...|  earthquake|            0.3|      0.36|              0.1|   5.0|automatic|    nc|\n|       38.8338318|       -122.8154984| 1.82|1.13|     md|22.0| 66.0| 0.01632|0.02| nc|nc73701236|2022-03-04 21:29:...|  earthquake|           0.19|      0.53|             0.14|   4.0|automatic|    nc|\n|       35.5966682|       -120.2713318|11.57|2.31|     md| 5.0|178.0|   0.159|0.01| nc|nc73701231|2022-03-04 21:26:...|  earthquake|           3.14|      3.51|             0.76|   3.0|automatic|    nc|\n|       35.9291667|       -117.6608333| 3.25|0.88|     ml| 9.0| 73.0| 0.02053|0.13| ci|ci40199696|2022-03-04 21:24:...|  earthquake|           0.33|      0.74|            0.055|  10.0|automatic|    ci|\n|           33.069|       -116.0461667| 7.75|1.54|     ml|35.0| 45.0|   0.119|0.25| ci|ci40199664|2022-03-04 20:02:...|  earthquake|            0.3|      1.08|            0.116|  28.0|automatic|    ci|\n|           33.467|       -116.4743333|  7.6|0.74|     ml|21.0| 65.0|  0.1019| 0.2| ci|ci40199656|2022-03-04 19:37:...|  earthquake|           0.31|      1.01|             0.21|  18.0|automatic|    ci|\n|          35.8475|       -117.6781667| 8.58|0.69|     ml|12.0| 86.0|  0.0728|0.06| ci|ci40199648|2022-03-04 19:26:...|  earthquake|           0.19|      0.57|            0.132|   7.0|automatic|    ci|\n|       33.8241667|            -117.48|-0.51|1.03|     ml|36.0| 45.0|  0.0299|0.18| ci|ci40199632|2022-03-04 18:42:...|quarry blast|           0.36|     31.61|            0.082|  16.0| reviewed|    ci|\n|       38.7941667|       -122.7473333| 2.13|1.37|     md|42.0| 46.0| 0.01163|0.04| nc|nc73701176|2022-03-04 21:22:...|  earthquake|           0.16|       0.3|            0.108|  12.0| reviewed|    nc|\n|             37.5|       -118.8758333|-0.51|1.28|     md|16.0|142.0| 0.09724|0.18| nc|nc73701171|2022-03-04 21:07:...|  earthquake|           0.54|     31.61|            0.214|  14.0| reviewed|    nc|\n|       36.1313333|       -117.8333333| 2.47|0.78|     ml|16.0|131.0| 0.02218| 0.1| ci|ci40199616|2022-03-04 18:45:...|  earthquake|           0.28|      0.21|            0.261|   7.0| reviewed|    ci|\n|       37.6456667|       -118.8631667| 4.29| 0.7|     md|11.0|114.0|  0.0265|0.07| nc|nc73701166|2022-03-04 19:24:...|  earthquake|            0.5|       0.7|            0.067|  10.0| reviewed|    nc|\n|           34.006|          -117.0245|-0.75|0.78|     ml|18.0|198.0|  0.1823|0.22| ci|ci40199608|2022-03-04 18:57:...|  earthquake|           0.66|     31.61|            0.172|  12.0| reviewed|    ci|\n|       34.0388333|       -117.0381667| 9.64|1.12|     ml|41.0| 91.0| 0.08188|0.18| ci|ci40199600|2022-03-04 18:46:...|  earthquake|           0.21|      0.48|            0.197|  19.0| reviewed|    ci|\n|           37.646|          -118.8605| 3.37|0.48|     md| 6.0|124.0| 0.02502|0.04| nc|nc73701161|2022-03-04 18:38:...|  earthquake|           0.76|      1.54|            0.133|   6.0| reviewed|    nc|\n|       37.6413333|       -118.8593333|  5.7|1.94|     md|18.0| 88.0| 0.02157|0.06| nc|nc73701156|2022-03-04 20:39:...|  earthquake|           0.39|      0.51|            0.165|  21.0| reviewed|    nc|\n|       38.8193321|       -122.8043365|  3.0| 1.0|     md|15.0| 45.0|0.006045|0.02| nc|nc73701151|2022-03-04 18:30:...|  earthquake|           0.31|       1.0|             0.21|   2.0|automatic|    nc|\n|       33.8491667|       -117.4978333| 2.29|1.72|     ml|57.0| 31.0| 0.05418|0.14| ci|ci40199592|2022-03-04 18:38:...|quarry blast|           0.17|      0.31|             0.17|  22.0| reviewed|    ci|\n|       34.0363333|       -117.0356667|12.88|2.12|     ml|79.0| 47.0| 0.06374|0.13| ci|ci40199584|2022-03-04 18:38:...|  earthquake|           0.13|      0.33|            0.194|  24.0| reviewed|    ci|\n|46.99366666666667|-120.36183333333334|-0.93|1.71|     ml|16.0| 92.0|   0.143|0.33| uw|uw61820151|2022-03-04 19:53:...|   explosion|           0.88|     31.61|0.139427161652421|  12.0| reviewed|    uw|\n+-----------------+-------------------+-----+----+-------+----+-----+--------+----+---+----------+--------------------+------------+---------------+----------+-----------------+------+---------+------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import *\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 59,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.ml.feature import StringIndexer\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 60,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "l=[\"magType\",\"net\",\"type\",\"source\"]\n#indexer=StringIndexer(inputCol=[\"magType\",\"net\",\"type\",\"source\"],outputCol=[\"magType1\",\"net1\",\"type1\",\"source1\"],handleInvalid=\"keep\",stringOrderType=\"frequencyDesc\")\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 61,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "indexer = [\nStringIndexer(inputCol=c, outputCol=\"{0}1\".format(c))\nfor c in l\n]",
			"metadata": {
				"trusted": true
			},
			"execution_count": 62,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.ml import Pipeline\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler\nfrom pyspark.sql.functions import unix_timestamp\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 63,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Convert the \"time\" column to the number of seconds since the Unix epoch\ndata = data.withColumn(\"time\", unix_timestamp(data[\"time\"]) / 1000)\nva=VectorAssembler(inputCols=[\"latitude\",\"longitude\",\"depth\",\"magType1\",\"net1\",\"mag\",\"nst\",\"time\"],outputCol=\"features\")\n\npipeline = Pipeline(stages=indexer + [va])\ndf_tfm=pipeline.fit(data).transform(data)\ntrain, test = df_tfm.randomSplit([0.7, 0.3])\nnum_rows_train = train.count()\nnum_cols_train = len(train.columns)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 64,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "print(\"Training:\",num_rows_train,\"x\",num_cols_train)\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 65,
			"outputs": [
				{
					"name": "stdout",
					"text": "Training: 25645 x 24\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "num_rows_test = test.count()\nnum_cols_test = len(test.columns)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 66,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "print(\"Training:\",num_rows_test,\"x\",num_cols_test)\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 67,
			"outputs": [
				{
					"name": "stdout",
					"text": "Training: 11128 x 24\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df_tfm.columns\nscaler = StandardScaler(inputCol='features', outputCol='scaled_features')\nscaler_model = scaler.fit(df_tfm)\ntrain=scaler_model.transform(df_tfm)\ntest=scaler_model.transform(test)\ntrain.show(3,False )",
			"metadata": {
				"trusted": true
			},
			"execution_count": 68,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+----+-------+----+-----+--------+----+---+----------+-----------+----------+---------------+----------+--------+------+---------+------+--------+----+-----+-------+------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n|latitude  |longitude   |depth|mag |magType|nst |gap  |dmin    |rms |net|id        |time       |type      |horizontalError|depthError|magError|magNst|status   |source|magType1|net1|type1|source1|features                                                    |scaled_features                                                                                                                                            |\n+----------+------------+-----+----+-------+----+-----+--------+----+---+----------+-----------+----------+---------------+----------+--------+------+---------+------+--------+----+-----+-------+------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n|38.7596664|-122.7196655|1.61 |1.24|md     |14.0|115.0|0.004494|0.04|nc |nc73701241|1646429.376|earthquake|0.3            |0.36      |0.1     |5.0   |automatic|nc    |1.0     |1.0 |0.0  |1.0    |[38.7596664,-122.7196655,1.61,1.0,1.0,1.24,14.0,1646429.376]|[2.81111197095991,-2.1680177566655177,0.03383931875399207,1.3154077708782064,0.5283346404054184,0.9871376891934288,0.7615650518271594,237.13561085745107]  |\n|38.8338318|-122.8154984|1.82 |1.13|md     |22.0|66.0 |0.01632 |0.02|nc |nc73701236|1646429.342|earthquake|0.19           |0.53      |0.14    |4.0   |automatic|nc    |1.0     |1.0 |0.0  |1.0    |[38.8338318,-122.8154984,1.82,1.0,1.0,1.13,22.0,1646429.342]|[2.8164909451135944,-2.1697107813981567,0.03825314293929538,1.3154077708782064,0.5283346404054184,0.8995690232165924,1.196745081442679,237.13560596042305] |\n|35.5966682|-120.2713318|11.57|2.31|md     |5.0 |178.0|0.159   |0.01|nc |nc73701231|1646429.216|earthquake|3.14           |3.51      |0.76    |3.0   |automatic|nc    |1.0     |1.0 |0.0  |1.0    |[35.5966682,-120.2713318,11.57,1.0,1.0,2.31,5.0,1646429.216]|[2.581710045453538,-2.1247644531773116,0.24318069439980633,1.3154077708782064,0.5283346404054184,1.8389419855135651,0.27198751850969977,237.13558781261338]|\n+----------+------------+-----+----+-------+----+-----+--------+----+---+----------+-----------+----------+---------------+----------+--------+------+---------+------+--------+----+-----+-------+------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\nonly showing top 3 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.ml.classification import LogisticRegression\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 69,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "log=LogisticRegression(featuresCol='scaled_features',labelCol='type1')\nlrmodel=log.fit(train)\nprediction=lrmodel.transform(test)\ntest.show(3)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 70,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------+---------+-----+----+-------+----+-----+------+----+---+------------+-----------+----------+---------------+----------+--------+------+--------+------+--------+----+-----+-------+--------------------+--------------------+\n|latitude|longitude|depth| mag|magType| nst|  gap|  dmin| rms|net|          id|       time|      type|horizontalError|depthError|magError|magNst|  status|source|magType1|net1|type1|source1|            features|     scaled_features|\n+--------+---------+-----+----+-------+----+-----+------+----+---+------------+-----------+----------+---------------+----------+--------+------+--------+------+--------+----+-----+-------+--------------------+--------------------+\n| 17.1625|  -68.232| 57.0|3.97|     md|23.0|258.0|1.3595|0.54| pr|pr2022081001|1647942.683|earthquake|           3.35|     10.75|     0.1|   7.0|reviewed|    pr|     1.0| 6.0|  0.0|    6.0|[17.1625,-68.232,...|[1.24474005280910...|\n| 17.9153| -66.8736| 13.0|3.66|     md|23.0|195.0|0.0601|0.15| pr|pr2022140000|1653041.725|earthquake|           0.57|      0.38|     0.1|  12.0|reviewed|    pr|     1.0| 6.0|  0.0|    6.0|[17.9153,-66.8736...|[1.29933817731047...|\n| 17.9366| -66.9785|  8.0|1.77|     md| 5.0|230.0|0.0735|0.17| pr|pr2022115001|1650864.001|earthquake|           1.07|      1.39|     0.0|   1.0|reviewed|    pr|     1.0| 6.0|  0.0|    6.0|[17.9366,-66.9785...|[1.30088299672051...|\n+--------+---------+-----+----+-------+----+-----+------+----+---+------------+-----------+----------+---------------+----------+--------+------+--------+------+--------+----+-----+-------+--------------------+--------------------+\nonly showing top 3 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.ml.evaluation import RegressionEvaluator\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 73,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "test.groupby(\"type\").count().show()\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 74,
			"outputs": [
				{
					"name": "stdout",
					"text": "+------------------+-----+\n|              type|count|\n+------------------+-----+\n|         explosion|   94|\n|      quarry blast|  147|\n|       other event|    1|\n|        earthquake|10885|\n|chemical explosion|    1|\n+------------------+-----+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "train.groupby(\"type\").count().show()\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 75,
			"outputs": [
				{
					"name": "stdout",
					"text": "+------------------+-----+\n|              type|count|\n+------------------+-----+\n|         explosion|  370|\n|      quarry blast|  501|\n|       other event|    2|\n|        earthquake|35899|\n|chemical explosion|    1|\n+------------------+-----+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Use the MulticlassClassificationEvaluator to evaluate the model's accuracy\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator",
			"metadata": {
				"trusted": true
			},
			"execution_count": 76,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "evaluator = MulticlassClassificationEvaluator(labelCol=\"type1\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(prediction)\nprint(\"Accuracy:\", accuracy)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 77,
			"outputs": [
				{
					"name": "stdout",
					"text": "Accuracy: 0.9758267433501079\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Select the \"prediction\" and \"label\" columns\npredictions_df = prediction.select([\"prediction\", \"type1\"])",
			"metadata": {
				"trusted": true
			},
			"execution_count": 79,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Convert the predictions and labels to Pandas dataframes for easier inspection\npredictions_pd = predictions_df.toPandas()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 80,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Print the first 10 predictions and their corresponding true labels\nprint(predictions_pd.head(10))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 81,
			"outputs": [
				{
					"name": "stdout",
					"text": "   prediction  type1\n0         0.0    0.0\n1         0.0    0.0\n2         0.0    0.0\n3         0.0    0.0\n4         0.0    0.0\n5         0.0    0.0\n6         0.0    0.0\n7         0.0    0.0\n8         0.0    0.0\n9         0.0    0.0\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Set the hyperparameters for the logistic regression model\nlr = LogisticRegression(labelCol='type1', featuresCol='features')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 82,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Fit the model to the training data\nlr_model = lr.fit(train)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 83,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Make predictions on the test data\npredictions = lr_model.transform(test)\n# Save the model to a file\n#lr_model.save(\"logistic_regression_model1\")\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 84,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Load the saved model\n#loaded_model = LogisticRegression.load(\"/content/logistic_regression_model1\")",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "\naccuracy = evaluator.evaluate(predictions)\nprint(\"Accuracy:\", accuracy)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 85,
			"outputs": [
				{
					"name": "stdout",
					"text": "Accuracy: 0.9757368799424874\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.ml.classification import RandomForestClassifier\nrand=RandomForestClassifier(featuresCol='scaled_features',labelCol='type1')\nrmodel=rand.fit(train)\npredictionrand=rmodel.transform(test)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 86,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nevaluator = MulticlassClassificationEvaluator(labelCol=\"type1\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictionrand)\nprint(\"Accuracy:\", accuracy)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 87,
			"outputs": [
				{
					"name": "stdout",
					"text": "Accuracy: 0.9883177570093458\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Select the \"prediction\" and \"label\" columns\npredictions_df = predictionrand.select([\"prediction\", \"type1\"])\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 88,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Convert the predictions and labels to Pandas dataframes for easier inspection\npredictions_pd = predictions_df.toPandas()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 90,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Print the first 10 predictions and their corresponding true labels\nprint(predictions_pd.head(10))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 91,
			"outputs": [
				{
					"name": "stdout",
					"text": "   prediction  type1\n0         0.0    0.0\n1         0.0    0.0\n2         0.0    0.0\n3         0.0    0.0\n4         0.0    0.0\n5         0.0    0.0\n6         0.0    0.0\n7         0.0    0.0\n8         0.0    0.0\n9         0.0    0.0\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Set the hyperparameters for the logistic regression model\nregrand = RandomForestClassifier(labelCol='type1', featuresCol='features',numTrees=100,maxDepth=5)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 92,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Fit the model to the training data\nregmodel = regrand.fit(train)\n\n# Make predictions on the test data\npredictions = regmodel.transform(test)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 93,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "accuracy = evaluator.evaluate(predictions)\nprint(\"Accuracy:\", accuracy)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 94,
			"outputs": [
				{
					"name": "stdout",
					"text": "Accuracy: 0.985981308411215\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 95,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Define the hyperparameters to tune\nhyperparameters = [\n    {'regParam': [0.1, 0.01, 0.001], 'elasticNetParam': [0.0, 0.5, 1.0]},\n    {'regParam': [0.1, 0.01, 0.001], 'elasticNetParam': [0.0, 0.5, 1.0], 'maxIter': [10, 50, 100]}\n]\nparam_grid = ParamGridBuilder().addGrid(log.regParam, hyperparameters[0]['regParam'])\\\n                               .addGrid(log.elasticNetParam, hyperparameters[0]['elasticNetParam'])\\\n                               .build()\ncv = CrossValidator(estimator=log, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=2)\nmodel = cv.fit(train)\nmodel.params\nmodel.bestModel\npredictions = model.transform(test)\n\naccuracy = evaluator.evaluate(predictions)\nprint(\"Accuracy: \", accuracy)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 96,
			"outputs": [
				{
					"name": "stdout",
					"text": "Accuracy:  0.9781631919482386\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}